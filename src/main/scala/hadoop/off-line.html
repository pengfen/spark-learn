离线数据分析流程

1. 需求分析

案例名称 ---> “网站或APP点击流日志数据挖掘系统”
[一般中型的网站(10W的PV以上)，每天会产生1G以上Web日志文件。大型或超大型的网站，可能每小时就会产生10G的数据量。
具体来说，比如某电子商务网站，在线团购业务。每日PV数100w，独立IP数5w。用户通常在工作日上午10:00-12:00和下午15:00-18:00访问量最大。
日间主要是通过PC端浏览器访问，休息日及夜间通过移动设备访问较多。网站搜索浏量占整个网站的80%，PC用户不足1%的用户会消费，移动用户有5%会消费。

对于日志的这种规模的数据，用HADOOP进行日志分析，是最适合不过的了。

案例需求描述
“Web点击流日志”包含着网站运营很重要的信息，通过日志分析，我们可以知道网站的访问量，哪个网页访问人数最多，哪个网页最有价值，广告转化率、
访客的来源信息，访客的终端信息等。

数据来源
本案例的数据主要由用户的点击行为记录
获取方式：在页面预埋一段js程序，为页面上想要监听的标签绑定事件，只要用户点击或移动到标签，即可触发ajax请求到后台servlet程序，
用log4j记录下事件信息，从而在web服务器（nginx、tomcat等）上形成不断增长的日志文件
58.215.204.118 - - [18/Sep/2013:06:51:35 +0000] "GET /wp-includes/js/jquery/jquery.js?ver=1.10.2 HTTP/1.1" 304 0 "http://blog.fens.me/nodejs-socketio-chat/" "Mozilla/5.0 (Windows NT 5.1; rv:23.0) Gecko/20100101 Firefox/23.0"
IP - - 时间 HTTP 状态码 0 访问地址 浏览器信息

2. 数据处理流程

流程图解析
本案例跟典型的BI系统极其类似，整体流程如下：
采集数据 ---> 数据预处理 ---> 导入Hive仓库 ---> ETL ---> 报表统计 ---> 结果导出到MySql ---> 数据可视化

但是，由于本案例的前提是处理海量数据，因而，流程中各环节所使用的技术则跟传统BI完全不同，后续课程都会一一讲解：
1)数据采集：定制开发采集程序，或使用开源框架FLUME
2)数据预处理：定制开发mapreduce程序运行于hadoop集群
3)数据仓库技术：基于hadoop之上的Hive
4)数据导出：基于hadoop的sqoop数据导入导出工具
5)数据可视化：定制开发web程序或使用kettle等产品
6)整个过程的流程调度：hadoop生态圈中的oozie工具或其他类似开源产品

c)将统计结果导入mysql
./sqoop export --connect jdbc:mysql://localhost:3306/weblogdb --username root --password root  --table t_display_xx  --export-dir /user/hive/warehouse/uv/dt=2014-08-03

3. 项目可视化
经过完整的数据处理流程后，会周期性输出各类统计指标的报表，在生产实践中，最终需要将这些报表数据以可视化的形式展现出来