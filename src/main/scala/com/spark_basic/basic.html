安装 spark
tar -zxvf spark-...tgz -C ~/app/

配置spark
cd $SPARK_HOME
cd conf/
mv spark-env.sh.template spark-env.sh
vi spark-env.sh
export JAVA_HOME=
export SPARK_MASTER_IP=ricky (注意)
export SPARK_MASTER_PORT=7077

zk接管服务就不配置 SPARK_MASTER_IP
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2 -Dspark.deploy.zookeeper.dir=/spark"

配置从节点
mv slaves.template slaves
vi slaves
node2.ricky

scp -r spark/ node2.ricky:~/app/spark
spark 启动
bin/spark-shell --master spark://ricky:7077 --executor-memory 512m --total-executor-cores 2

/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-shell \
--master spark://ricky:7077 \
--executor-memory 2g \
--total-executor-cores 2

参数说明：
--master spark://node1.itcast.cn:7077 指定Master的地址
--executor-memory 2g 指定每个worker可用内存为2G
--total-executor-cores 2 指定整个集群使用的cup核数为2个


scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@6892d403

sc.textFile("hdfs://ricky:9000/wc").flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_).sortBy(_._2, false).collect
sc.textFile("hdfs://ricky:9000/wc").flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_).sortBy(_._2, false).saveAtTextFile("hdfs://ricky:9000/out")

scala> val rdd1 = sc.parallelize(List(5, 6, 4, 7, 3, 8, 2, 9, 1, 10))
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> rdd1.map(_*10).sortBy(x => x, true)
res1: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[6] at sortBy at <console>:27

scala> rdd1.map(_*10).sortBy(x => x, true).collect
res2: Array[Int] = Array(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)

scala> rdd1.map(_*10).sortBy(x => x + "", true).collect
res3: Array[Int] = Array(10, 100, 20, 30, 40, 50, 60, 70, 80, 90)

scala> rdd1.map(_*10).sortBy(x => x + "", true).filter(_>10).collect
res4: Array[Int] = Array(100, 20, 30, 40, 50, 60, 70, 80, 90)

scala> val rdd4 = sc.parallelize(Array("a b c", "d e f", "h i j"))
rdd4: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[26] at parallelize at <console>:24

scala> rdd4.flatMap(_.split(" "))
res5: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:27

scala> res5.collect
res6: Array[String] = Array(a, b, c, d, e, f, h, i, j)

scala> val rdd5 = sc.parallelize(List(List("a b c", "a b b"), List("e f g", "a f g"), List("h i j", "a a b")))
rdd5: org.apache.spark.rdd.RDD[List[String]] = ParallelCollectionRDD[28] at parallelize at <console>:24

scala> rdd5.flatMap(_.flatMap(_.split(" "))).collect
res7: Array[String] = Array(a, b, c, a, b, b, e, f, g, a, f, g, h, i, j, a, a, b)

scala> val rdd6 = sc.parallelize(List(5, 6, 4, 7))
rdd6: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[30] at parallelize at <console>:24

scala> val rdd7 = sc.parallelize(List(1, 2, 3, 4))
rdd7: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[31] at parallelize at <console>:24

scala> val rdd8 = rdd6.union(rdd7)
rdd8: org.apache.spark.rdd.RDD[Int] = UnionRDD[32] at union at <console>:28

scala> rdd8.collect
res8: Array[Int] = Array(5, 6, 4, 7, 1, 2, 3, 4)

scala> val rdd9 = rdd6.intersection(rdd7)
rdd9: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[38] at intersection at <console>:28

scala> val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 2), ("kitty", 3)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[39] at parallelize at <console>:24

scala> rdd9.collect
res9: Array[Int] = Array(4)

scala> val rdd2 = sc.parallelize(List(("jerry", 9), ("tom", 6), ("shuke", 7)))
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[40] at parallelize at <console>:24

scala> rdd1.intersection(rdd2)
res11: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[46] at intersection at <console>:29

scala> res11.collect
res12: Array[(String, Int)] = Array()

scala> val rdd3 = rdd1.join(rdd2)
rdd3: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[49] at join at <console>:28

scala> rdd3.collect
res13: Array[(String, (Int, Int))] = Array((tom,(1,6)), (jerry,(2,9)))

scala> val rdd2 = sc.parallelize(List(("jerry", 9), ("tom", 8), ("shuke", 7), ("tom", 2)))
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[50] at parallelize at <console>:24

scala> val rdd3 = rdd1.join(rdd2)
rdd3: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[53] at join at <console>:28

scala> rdd3.collect
res14: Array[(String, (Int, Int))] = Array((tom,(1,8)), (tom,(1,2)), (jerry,(2,9)))

scala> val rdd3 = rdd1.rightOuterJoin(rdd2)
rdd3: org.apache.spark.rdd.RDD[(String, (Option[Int], Int))] = MapPartitionsRDD[56] at rightOuterJoin at <console>:28

scala> rdd3.collect
res15: Array[(String, (Option[Int], Int))] = Array((tom,(Some(1),8)), (tom,(Some(1),2)), (jerry,(Some(2),9)), (shuke,(None,7)))

scala> val rdd3 = rdd1 union rdd2
rdd3: org.apache.spark.rdd.RDD[(String, Int)] = UnionRDD[57] at union at <console>:28

scala> rdd3.collect
res16: Array[(String, Int)] = Array((tom,1), (jerry,2), (kitty,3), (jerry,9), (tom,8), (shuke,7), (tom,2))

scala> rdd3.groupByKey
res17: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[58] at groupByKey at <console>:31

scala> val rdd4 = rdd3.groupByKey
rdd4: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[59] at groupByKey at <console>:30

scala> rdd4.collect
res18: Array[(String, Iterable[Int])] = Array((tom,CompactBuffer(1, 8, 2)), (jerry,CompactBuffer(2, 9)), (shuke,CompactBuffer(7)), (kitty,CompactBuffer(3)))

scala> val rdd1 = sc.parallelize(List("tom", "jerry"))
rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[62] at parallelize at <console>:24

scala> val rdd2 = sc.parallelize(List("tom", "kitty", "shuke"))
rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[63] at parallelize at <console>:24

scala> val rdd3 = rdd1.cartesian(rdd2)
rdd3: org.apache.spark.rdd.RDD[(String, String)] = CartesianRDD[64] at cartesian at <console>:28

scala> rdd3.collect
res21: Array[(String, String)] = Array((tom,tom), (tom,kitty), (tom,shuke), (jerry,tom), (jerry,kitty), (jerry,shuke))

scala> val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5), 2)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[65] at parallelize at <console>:24

scala> val rdd2 = rdd1.reduce(_+_)
rdd2: Int = 15

scala> rdd1.count
res22: Long = 5

scala> rdd1.take(2)
res25: Array[Int] = Array(1, 2)

scala> rdd1.top(2)
res26: Array[Int] = Array(5, 4)

scala> rdd1.first
res27: Int = 1

scala> rdd1.takeOrdered(3)
res28: Array[Int] = Array(1, 2, 3)
