#调试Master，在master节点的spark-env.sh中添加SPARK_MASTER_OPTS变量
export SPARK_MASTER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10000"
#启动Master
sbin/start-master.sh


#调试Worker，在worker节点的spark-env.sh中添加SPARK_WORKER_OPTS变量
export SPARK_WORKER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10001"
#启动Worker
sbin/start-slave.sh 1 spark://ricky:7077

#调试spark-submit + app
bin/spark-submit \
--class spark.basic.WordCount \
--master spark://ricky:7077 \
--driver-java-options "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10002" \
/home/ricky/spark-jar/spark-learn-1.0.jar \
hdfs://ricky:9000/words.txt hdfs://ricky:9000/out2

#调试spark-submit + app + executor
bin/spark-submit \
--class spark.basic.WordCount \
--master spark://ricky:7077 \
--conf "spark.executor.extraJavaOptions=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10003" \
--driver-java-options "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10002" \
/home/ricky/spark-jar/spark-learn-1.0.jar \
hdfs://ricky:9000/words.txt hdfs://ricky:9000/out2



spark 远程调试
--------------------------------------------------------------------------------------------------------------

start-all.sh -> start-master.sh  -> start-slaves.sh

//Master启动的脚本
start-master.sh  -> spark-daemon.sh start org.apache.spark.deploy.master.Master


//Worer的启动过程
salves.sh   ->  通过读取slaves 通过ssh的方式启动远端的worker
spark-daemon.sh start org.apache.spark.deploy.worker.Worker


Master和Worker是两个Java进程  他们启动的时候会加载一些参数 spark-env.sh这里边的环境变量


如何是使用远程debug

在Master端的spark-env.sh文件中添加如下参数
export SPARK_MASTER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10000"

start-master.sh
执行完这个脚本
jps
4828 -- main class information unavailable
4860 Jps

通过一个IDE  建立一个remote application
172.16.0.11 10000
在本地的代码打断点
debug按钮开始调试

---------------------------------------------------------------------------------------------------------

在Worker所在的配置文件中添加一个环境变量
export SPARK_WORKER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10001"
start-slave.sh spark://node-1.itcast.cn:7077

执行jps命令
2891 -- main class information unavailable
2923 Jps

用一个IDE工具连接 建立一个remote application
172.16.0.12 10001
在本地的代码打断点
debug按钮开始调试

---------------------------------------------------------------------------------------------------------
Debug app （--driver-java-options）

bin/spark-submit --class cn.itcast.spark.WC --master spark://node-1.itcast.cn:7077 --driver-java-options "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10002" /root/bigdata-2.0.jar hdfs://node-1.itcast.cn:9000/words.txt hdfs://node-1.itcast.cn:9000/wordsout

任务提交流程
spark-submit --class cn.itcast.spark.WordCount

bin/spark-clas -> org.apache.spark.deploy.SparkSubmit 调用这个类的main方法

doRunMain方法中传进来一个自定义spark应用程序的main方法class cn.itcast.spark.WordCount

通过反射拿到类的实例的引用mainClass = Utils.classForName(childMainClass)

在通过反射调用class cn.itcast.spark.WordCount的main方法


用一个IDE工具连接 建立一个remote application
172.16.0.13 10002
在本地的代码打断点
debug按钮开始调试
